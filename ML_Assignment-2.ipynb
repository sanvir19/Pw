{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f330fc4a-94e4-4eef-81f1-c98e0940349a",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "001fc714-30ca-499b-a2f2-5d4c642f36bd",
   "metadata": {},
   "source": [
    "Overfitting in Machine Learning: Overfitting occurs when a machine learning model learns the training data too well \n",
    "                                 on the training data but poorly on new, unseen data (test data). \n",
    "    \n",
    "Consequences of Overfitting: \n",
    "    Poor Generalization\n",
    "    High Variance\n",
    "    Loss of Insights\n",
    "    \n",
    "Mitigation of Overfitting:\n",
    "    More Data\n",
    "    Feature Selection/Engineering\n",
    "    Regularization\n",
    "    Cross-Validation\n",
    "    Simpler Models\n",
    "    Early Stopping\n",
    "    Ensemble Methods\n",
    "    \n",
    "Underfitting in Machine Learning: Underfitting occurs when a model is too simple to capture the underlying\n",
    "                                  patterns in the training data. As a result, it performs poorly not only \n",
    "                                  on the training data but also on new, unseen data\n",
    "        \n",
    "Consequences of Underfitting:\n",
    "    Poor Training and Test Performance\n",
    "    Biased Representations\n",
    "    \n",
    "Mitigation of Underfitting:\n",
    "    More Features\n",
    "    Complex Models\n",
    "    Hyperparameter Tuning\n",
    "    Ensemble Methods\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4462e4-baf7-4d21-b197-6eed0dfe9173",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "raw",
   "id": "69cb3925-ac27-4765-af41-0cdfbd15eda1",
   "metadata": {},
   "source": [
    "List reduce overfitting using some technique\n",
    "\n",
    "More Data\n",
    "Cross-Validation\n",
    "Feature Selection\n",
    "Regularization\n",
    "Simpler Model Architectures\n",
    "Early Stopping\n",
    "Dropout\n",
    "Ensemble Methods\n",
    "Data Augmentation\n",
    "Hyperparameter Tuning\n",
    "Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93547910-b408-4a6e-bdc8-631600f80503",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3519285d-dd9e-414b-8c4d-1caef6a7e7e9",
   "metadata": {},
   "source": [
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the training data. As a result, it performs poorly not only on the training data but also on new, unseen data.\n",
    "\n",
    "Here are some scenarios where underfitting can occur in machine learning:\n",
    "    Insufficient Model Complexity\n",
    "    Limited Features\n",
    "    Small Training Dataset\n",
    "    High Regularization\n",
    "    Ignoring Domain Knowledge\n",
    "    Misaligned Complexity\n",
    "    Early Stopping\n",
    "    Ignoring Outliers\n",
    "    Ignoring Temporal Dependencies\n",
    "    Inadequate Feature Scaling\n",
    "    Choosing the Wrong Algorithm\n",
    "    Over-Pruning Decision Trees\n",
    "    Ignoring Nonlinear Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadd98f-b1ac-45a7-830c-36b9e1783965",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0888d85-5705-4297-b108-3b01288704cc",
   "metadata": {},
   "source": [
    "Bias: Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model.A model with high bias oversimplifies the problem and may not be able to capture the underlying patterns in the data.\n",
    "\n",
    "Variance: Variance refers to the model's sensitivity to small fluctuations or changes in the training data.\n",
    "A model with high variance fits the training data closely and captures noise, but it may fail to generalize well to new data.\n",
    "\n",
    "\n",
    "Relationship:\n",
    "There is an inverse relationship between bias and variance. Increasing the complexity of a model (e.g., using more features or a more complex algorithm) tends to decrease bias but increase variance, and vice versa.\n",
    "Bias and variance are both types of errors that contribute to a model's total error. The goal is to find a balance that minimizes the total error on new, unseen data.\n",
    "\n",
    "\n",
    "Mitigating Bias-Variance Tradeoff:\n",
    "To reduce bias, consider using more complex models, adding relevant features, or fine-tuning hyperparameters.\n",
    "To reduce variance, gather more data, apply regularization techniques, or use simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c293d-5c9e-48da-ae70-4b33dce67de7",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12ee729d-45bd-4010-9f51-bf7ee9d6a722",
   "metadata": {},
   "source": [
    "Detecting Overfitting:\n",
    "    Validation Curves\n",
    "    Learning Curves\n",
    "    Cross-Validation\n",
    "    Early Stopping\n",
    "    Validation Set Performance\n",
    "    Feature Importance\n",
    "    \n",
    "Detecting Underfitting:\n",
    "    Validation Curves\n",
    "    Learning Curves\n",
    "    Cross-Validation\n",
    "    Model Complexity\n",
    "    Feature Importance\n",
    "    \n",
    "Mitigation Strategies:\n",
    "\n",
    "For overfitting: Reduce model complexity, apply regularization techniques (L1, L2, dropout), gather more data,\n",
    "or use simpler algorithms.For underfitting: Increase model complexity, engineer relevant features, \n",
    "fine-tune hyperparameters, or try more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06e028-d912-4a12-baf0-f4ae21bb49b6",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d2aa906-2453-4de3-9348-86d72746fa11",
   "metadata": {},
   "source": [
    "Bias: \n",
    "Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias models tend to underfit the data and make overly simplistic assumptions. They can't capture the underlying patterns and have systematic errors, leading to poor training and test performance.\n",
    "\n",
    "Variance: \n",
    "Variance refers to the model's sensitivity to small fluctuations or changes in the training data. High variance models fit the training data very closely but may not generalize well to new data. They capture noise and randomness in the training data, leading to erratic and inconsistent predictions on unseen data.\n",
    "\n",
    "\n",
    "Examples of High Bias and High Variance Models:\n",
    "\n",
    "\n",
    "\n",
    "High Bias (Underfitting):\n",
    "\n",
    "Linear Regression: A simple linear regression model that attempts to fit a complex, nonlinear relationship between variables may have high bias. It oversimplifies the relationship and misses important patterns.\n",
    "Underfit Decision Tree: If a decision tree is too shallow, it won't be able to capture complex decision boundaries, resulting in high bias.\n",
    "\n",
    "\n",
    "\n",
    "High Variance (Overfitting):\n",
    "\n",
    "Complex Neural Network: A deep neural network with many layers can fit the training data very closely, capturing noise and leading to high variance.\n",
    "k-Nearest Neighbors (k-NN) with k=1: A k-NN model with k=1 can memorize the training data and perform poorly on new data, exhibiting high variance.\n",
    "High-Degree Polynomial Regression: A polynomial regression model with a very high degree can fit the training data perfectly but produce erratic predictions on new data.\n",
    "\n",
    "\n",
    "\n",
    "Differences in Performance:\n",
    "\n",
    "High Bias (Underfitting):\n",
    "\n",
    "Training Performance: Poor, as the model fails to capture the data's underlying patterns.\n",
    "Validation/Test Performance: Poor, as the model can't generalize well to new data.\n",
    "Bias: High (systematic errors).\n",
    "Variance: Low.\n",
    "\n",
    "High Variance (Overfitting):\n",
    "\n",
    "Training Performance: High, as the model fits the training data closely.\n",
    "Validation/Test Performance: Poor, as the model fails to generalize, leading to erratic predictions.\n",
    "Bias: Low.\n",
    "Variance: High (sensitive to fluctuations in data).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Balancing Bias and Variance:\n",
    "\n",
    "The goal is to find a balance between bias and variance to achieve a model that generalizes well to new data. Regularization techniques, cross-validation, and ensembling methods (e.g., bagging and boosting) are often used to mitigate the tradeoff and optimize model performance. It's important to understand the characteristics of your data and choose an appropriate model complexity to strike this balance effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928bf7f-352c-4d34-97f3-5c26f7f012de",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c949bb-05a2-454b-906b-8422e194321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 Regularization (Lasso)\n",
    "L2 Regularization (Ridge)\n",
    "Elastic Net Regularization\n",
    "Dropout\n",
    "Early Stopping\n",
    "Cross-Validation\n",
    "Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
